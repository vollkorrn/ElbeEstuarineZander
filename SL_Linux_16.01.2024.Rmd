---
title: "SL_Linux_16.01_Paper.2024"
output: html_document
date: "2024-01-16"
---

# 1 SSU 16S

The data have been created on three different MiSeq Runs, 
Batch1 and its Replication Batch1_reseq
Batch2
Samples from sampling groups are spread across runs and did not show obvious batch effects


Steps
Step1 Trim-galore for Illumina adapers and general quality <- Optional
Step2 FiltN and 16SPrimers and create Quality Plots
Step3 Run dada2 Filter-Chimera-SeqError-Taxa-Pipeline once in full to have it
Step4 Merge the SeqTabs from different sequencing runs and do

## 1.2 Trimming

I trim the reads initially to filter Illumina adapter remnants and bad quality reads overall

```{r}
############################################################
#Step1 Trim-galore for Illumina adapers and general quality#
############################################################
#############
#Trim-Galore#
#############
#!/bin/bash
#SBATCH --job-name=Trim
#SBATCH --nodes=1
#SBATCH --partition=big
#SBATCH --tasks-per-node=16
#SBATCH --time=12:00:00
#SBATCH --export=NONE
#SBATCH --error=error_Trim.txt
#SBATCH --mail-user=raphael.koll@uni-hamburg.de
#SBATCH --mail-type=ALL
source /sw/batch/init.sh
source /usw/anaconda3/etc/profile.d/conda.sh
conda activate CondaTrim
for f1 in *_R1_001.fastq.gz
do
        f2=${f1%%_R1_001.fastq.gz}"_R2_001.fastq.gz"
        trim_galore --paired --fastqc -o trim_galore/ $f1 $f2
done
#Default settings https://github.com/FelixKrueger/TrimGalore/blob/master/Docs/Trim_Galore_User_Guide.md

#########
#MultiQC#
#########
source /usw/anaconda3/etc/profile.d/conda.sh
conda activate MambaRNAseq
multiqc .

rm -r multiqc_data
mv multiqc_report Batch2__l6fbp_multiqc_report
```

## 1.3 Primer Removal

Now I specifically remove the 16S Primers 
FWD = "CCTACGGGAGGCAGCAG"
REV = "GGACTACHVGGGTWTCTAAT"

```{r}
#!/bin/bash
#SBATCH --job-name=dada2
#SBATCH --nodes=1
#SBATCH --tasks-per-node=16
#SBATCH --partition=std #not necessary on a big nÃ³de i guess but the usual ones were blocked
#SBATCH --time=12:00:00
#SBATCH --export=NONE
#SBATCH --error=error_filterNandPrimer-Batch2_Run1-01.08.23.txt
#SBATCH --mail-user=raphael.koll@uni-hamburg.de
#SBATCH --mail-type=ALL
source /sw/batch/init.sh
source /usw/anaconda3/etc/profile.d/conda.sh
conda activate MambaDada2
cd /work/16S/Batch1-ktfrl/01.08.23
R -q -f cutadapt-filterNandPrimer-01.08.23.R > filterNandPrimer-01.08.23.out 2>&1

######################################
#cutadapt-filterNandPrimer-01.08.23.R#
######################################

Date <- "01.08.23"
#Batch1
path <- "/work/16S/Batch1-ktfrl/01.08.23/trim_galore"
pathOut <- "/work/16S/Batch1-ktfrl/01.08.23/trim_galore"

pathTrainsets <-"/work/16S/trainsets"

library(dada2)
library(ShortRead)
library(ggplot2)
head(list.files(path))
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="R1_001_val_1.fq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="R2_001_val_2.fq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

##cutadapt - identify and remove primer sequences: #Primer sequences: 341F and 806R used 
FWD = "CCTACGGGAGGCAGCAG"
REV = "GGACTACHVGGGTWTCTAAT"

##########START cutadapt PRIMER SCREENING AND REMOVAL##################
allOrients <- function(primer) {
  # Create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
               RevComp = Biostrings::reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients

fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))

##Takes some minutes when not running via batch system##
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = FALSE)

primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]),
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]),
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]),
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
#Forward Complement Reverse RevComp
#FWD.ForwardReads   27814          0       0       0
#FWD.ReverseReads       0          0       0      21
#REV.ForwardReads       0          0       0       2
#REV.ReverseReads   26861          0       0       0

cutadapt <- paste0("/usw/anaconda3/envs/CondaTrim/bin/cutadapt") #Conda path to cutadapt
system2(cutadapt, args = "--version") # Run shell commands from R

path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC, "--minimum-length 100")
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC, "--minimum-length 100")
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}
##TAKES some minutes when not run via batch##
path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC,"--minimum-length 100")
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC,"--minimum-length 100")
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]),
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]),
      REV.ForwardReads = sapply(REV.orients, primerHits,fn = fnFs.cut[[1]]),
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))

# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "R1_001_val_1.fq", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R2_001_val_2.fq", full.names = TRUE))

# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)

############FINISH CUTADAPT PRIMER REMOVAL######################
```

## 1.4 Dada2 QualityPlot

```{r}
############ Plot selected QualityPlots #########################
library(dada2)
library(ShortRead)
library(ggplot2)
head(list.files(path))
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
path.cut <- file.path(path, "cutadapt")
head(list.files(path.cut))
cutFs <- sort(list.files(path.cut, pattern = "R1_001_val_1.fq", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R2_001_val_2.fq", full.names = TRUE))
for(i in 1:length(cutFs[c(1,10,20,50,100,110,150)])){
    plot.quals <- plotQualityProfile(cutFs[[i]])
    g <- gsub( "_.*$", "", basename(cutFs[[i]]))
    ggsave(plot.quals, filename = paste(g,"F.png", sep="_"), path = pathOut, device='png', dpi=300, width = 8,
    height = 6)}
for(i in 1:length(cutRs[c(1,10,20,50,100,110,150)])){
    plot.quals <- plotQualityProfile(cutRs[[i]])
    g <- gsub( "_.*$", "", basename(cutRs[[i]]))
    ggsave(plot.quals, filename = paste(g,"R.png", sep="_"), path = pathOut, device='png', dpi=300, width = 8,
    height = 6)}
```

## 1.5 PreRun for seq length determination

I ran the dada2 pipeline below several times with different settings
Especially the seqlength has to be evaluated
truncLen=c(270,190)
truncLen=c(270,200) #Turns out to be the best -> dada2-pipeline-470-02.08.23.R
truncLen=c(270,210)

```{r}
###############################################################################
#Step3 Run dada2 Filter-Chimera-SeqError-Taxa-Pipeline once in full to have it#
###############################################################################
#Steps done before:
    #Trim-Galore cut out Illumina adapters (in about 30% of the reads)
########
#Batch2#
########
#truncLen=c(270,190)

# "GCAU21BBEB1" 28486 27300 27145 27227 26833 26347
# "GCAU21BBEB2" 20615 19723 19677 19703 19578 19276
# "GCAU21BBEB3" 31033 29541 29094 29382 28326 27361

# 276   288   318   331   335   346   351   360   364   369   371   373   374
#   1     1     1     1     1     1     1     1     1     1     1     5     9
# 375   376   381   382   384   385   386   387   388   389   390   391   392
#  14    14     2     1     5     4     8     8     7     5     2     3     6
# 393   394   395   396   397   398   399   400   401   402   403   404   405
#   6     2     1    15    10    27    28     7    19   103   155  4238  2264
# 406   407   408   409   410   411   412   413   414   415   416   417   418
# 999   849   386  1405   124   129    51    48    10    26    76    24    68
# 419   420   421   422   423   424   425   426   427   428   429   430   431
# 564   135   536   265   482  4333   212   182   117  2062 28997  7981   254
# 432   433   434   435   436   437   438   439   440   441   442   443   444
#  17     1     1    12    29     3     9     6     7   159     2     4    20
# 445   446   447   448
#  49   955   253     9

#truncLen=c(270,200)

# "GCAU21BBEB1" 28486 27222 27067 27158 26725 26258
# "GCAU21BBEB2" 20615 19637 19591 19596 19470 19168
# "GCAU21BBEB3" 31033 29387 28943 29213 28134 27168

# 276   288   318   331   335   346   351   360   364   369   373   374   375
#   1     1     1     1     1     1     1     1     1     1     5     9    14
# 376   381   382   384   385   386   387   388   389   390   391   392   393
#  14     2     1     5     5     8     8     7     5     2     3     6     6
# 394   395   396   397   398   399   400   401   402   403   404   405   406
#   2     1    16    10    27    28     7    20   104   157  4230  2244  1000
# 407   408   409   410   411   412   413   414   415   416   417   418   419
# 826   337  1314   115   131    49    44    10    25    76    21    64   432
# 420   421   422   423   424   425   426   427   428   429   430   431   432
# 140   449   230   458  3415   199   168   110  1801 25333  6771   236    14
# 433   434   435   436   437   438   439   440   441   442   443   444   445
#   2     1     1    19     3     9     2     6   111     1     3    20    38
# 446   447   448   449   450   451   452   453   455   458
# 743   139    10     7    12    19    58     3     1     1

#truncLen=c(280,200)

# "GCAU21BBEB1" 28486 13843 13707 13781 13440 13293
# "GCAU21BBEB2" 20615 9963 9922 9934 9832 9759
# "GCAU21BBEB3" 31033 13050 12688 12934 12115 11848

# 288   335   346   351   364   373   374   375   376   381   382   384   386
#   1     1     1     1     1     4     6     9     7     2     1     4     4
# 387   388   389   390   391   392   393   394   395   396   397   398   399
#   4     3     2     2     3     6     5     2     1     9     6    20    22
# 400   401   402   403   404   405   406   407   408   409   410   411   412
#   8     5    37   109  2931  1741   731   589   175   723    76    73    42
# 413   414   415   416   417   418   419   420   421   422   423   424   425
#  30     7    23    60    16    48   264   111   302   187   335  2063   144
# 426   427   428   429   430   431   432   433   434   435   436   437   438
# 114    97  1321 13713  3505   173    13     2     1     1    16     3     9
# 439   440   441   442   443   444   445   446   447   448   449   450   453
#   1     4   105     8     3     7    34   604   118    29    10    18     1
# 459   461   462   463   465   466   468
#   6     1     3     1    14     3     1

########
#Batch1#
########

#truncLen=c(270, 190)

# 272   273   274   277   280   281   287   292   295   296   297   298   300
#   3     1     2     1     1     1     1     4     1     1     1     1     1
# 305   306   309   310   315   317   318   319   321   322   328   330   333
#   2     1     2     1     1     1     1     1     1     2     1     1     1
# 335   337   338   340   344   346   347   351   360   362   364   365   367
#   1     1     1     1     1     3     2     1     1     1     2     5     2
# 369   372   373   374   375   376   377   380   381   382   384   385   386
#   1     1     1    30    45    16     1     3     1     2     6     7    27
# 387   388   389   390   391   392   393   394   395   396   397   398   399
#  11    17     5     1     3     1     1     5     3    26    22    39    41
# 400   401   402   403   404   405   406   407   408   409   410   411   412
#   8    69   145   188  6600  3124  1230  1387   702  3082   266   184    50
# 413   414   415   416   417   418   419   420   421   422   423   424   425
#  46    41    39    98    28   126   506   137   415   268   628  6067   356
# 426   427   428   429   430   431   432   433   434   435   437   439   440
# 373   137  2655 23408  8938   666    76     1     5     2     4    10     4
# 441   442   443   444   445   446   447   448   449   450   451   452   453
#  10    16     6     1    15    42    79   100    28    48    10     9     3
# 455   456
#   1     1

#truncLen=c(280,200)

# "GCSP22BBEB1" 33516 23334 23192 23246 21862 21365
# "GCSP22BBEB3" 56419 40294 39882 40133 37327 35385
# "GCSP22BBEB5" 37299 28810 28728 28719 27016 25886

# 272   273   274   277   280   281   287   292   295   296   297   298   300
#   3     1     2     1     1     1     1     4     1     1     1     1     1
# 305   306   309   310   315   317   318   319   321   322   328   330   333
#   2     1     2     1     1     1     1     1     1     2     1     1     1
# 335   337   338   340   344   346   347   351   360   362   364   365   367
#   1     1     1     1     1     3     2     1     1     1     2     2     2
# 369   372   373   374   375   376   377   380   381   382   384   385   386
#   1     1     1    29    42    13     1     2     1     2     5     5    22
# 387   388   389   390   391   392   393   394   395   396   397   398   399
#  11    15     5     1     3     1     1     5     3    26    21    37    39
# 400   401   402   403   404   405   406   407   408   409   410   411   412
#   7    48   126   171  5610  2774  1120  1260   568  2573   209   168    40
# 413   414   415   416   417   418   419   420   421   422   423   424   425
#  49    36    34    86    29   107   454   118   368   233   581  5647   319
# 426   427   428   429   430   431   432   433   434   435   437   439   440
# 299   124  2257 19737  7462   551    61     1     5     2     5     7     3
# 441   442   443   444   445   446   447   448   449   450   451   452   453
#  10    12     6     1    12    43    90    83    19    27     1    12     1
# 455   456   464   465   466   468
#   1     1     1     1     2     1


##############
#Batch1-reseq#
##############

#truncLen=c(270, 190)

# "GCSP22BBEB1" 19004 17077 16949 17017 16668 16330
# "GCSP22BBEB3" 34286 31180 30817 31061 29950 28338
# "GCSP22BBEB5" 22353 20392 20311 20346 20105 19101

# 272   274   292   295   296   298   305   306   309   317   318   321   322
#    2     1     2     1     1     1     2     1     2     2     1     1     1
#  325   327   328   333   335   337   338   340   344   364   365   367   372
#    1     1     1     1     1     1     1     1     1     1     2     4     1
#  373   374   375   376   378   380   381   382   383   384   385   386   387
#    2    17    31     8     1     2     1     2     1     6     3     7     6
#  388   389   390   392   393   394   395   396   397   398   399   400   401
#    6     3     1     1     1     1     2    19    12    24    19     4    45
#  402   403   404   405   406   407   408   409   410   411   412   413   414
#   94   125  4494  2234   839   954   463  2142   209   134    40    38    30
#  415   416   417   418   419   420   421   422   423   424   425   426   427
#   28    87    22    98   406   110   330   236   502  5642   287   272   130
#  428   429   430   431   432   433   434   435   436   437   438   439   440
# 2284 19685  7661   666    58     3     4     4     1     2    10     8     2
#  441   442   443   444   445   446   447   448
#   16    20    18    17    41   122    75    86

#truncLen=c(270,200)

# "GCSP22BBEB1" 19004 16432 16304 16369 15988 15667
# "GCSP22BBEB3" 34286 30021 29674 29868 28547 27109
# "GCSP22BBEB5" 22353 19925 19841 19868 19592 18599

# 272   274   290   292   295   296   298   305   306   309   317   318   321
#    2     1     1     1     1     1     1     2     1     2     2     1     1
#  322   325   327   328   333   335   337   340   344   364   365   367   373
#    1     1     1     1     1     1     1     1     1     1     2     2     2
#  374   375   376   378   380   381   382   384   385   386   387   388   389
#   17    31     9     1     1     1     2     6     3     9     6     6     2
#  390   392   393   394   395   396   397   398   399   400   401   402   403
#    1     1     1     1     2    20    11    24    19     3    45    93   130
#  404   405   406   407   408   409   410   411   412   413   414   415   416
# 4220  2078   802   900   350  1754   185   113    33    30    27    20    70
#  417   418   419   420   421   422   423   424   425   426   427   428   429
#   20    76   304    99   282   176   422  3894   237   195   100  1627 14291
#  430   431   432   433   434   435   436   437   439   440   441   442   443
# 5358   430    50     5     3     2     1     5     6     1    18    15    21
#  444   445   446   447   448   449   450   451   452   453   455   456
#    5    19    98    69    75    15    51    28     8    17     1     1

#truncLen=c(280,200)

# 292  301  305  306  309  318  321  322  327  328  335  340  344  365  367  373
#    1    1    2    1    1    1    1    1    1    1    1    1    1    1    2    2
#  374  375  376  380  381  382  384  386  387  388  389  392  393  394  396  397
#   11   18    2    1    1    2    1    1    2    4    1    1    1    1    7    3
#  398  399  400  401  402  403  404  405  406  407  408  409  410  411  412  413
#   20   15    2   12   18   54 2481 1349  544  538  184 1052   52   42   19   22
#  414  415  416  417  418  419  420  421  422  423  424  425  426  427  428  429
#   20   17   43   10   49  179   74  183  130  289 1905  147  105   82 1060 8751
#  430  431  432  433  434  435  436  437  439  440  441  442  443  444  445  446
# 2808  252   37    2    2    1    1    2    3    4   15    9   19    4   18   82
#  447  448  449  450  451  452  453  455  462  465  468
#   51   45    2   44   26    3   14    1    1    1    1

```

## 1.6 Dada2

```{r}
#!/bin/bash
#SBATCH --job-name=dada2
#SBATCH --nodes=1
#SBATCH --tasks-per-node=16
#SBATCH --partition=stl 
#SBATCH --time=7-00:00:00
#SBATCH --export=NONE
#SBATCH --error=dada2-pipeline-470-02.08.23.txt
#SBATCH --mail-user=raphael.koll@uni-hamburg.de
#SBATCH --mail-type=ALL
source /sw/batch/init.sh
source /usw/anaconda3/etc/profile.d/conda.sh
conda activate MambaDada2
cd /work/16S/Batch1_Run1/02.08.23
R -q -f dada2-pipeline-470-02.08.23.R  > dada2-pipeline-470-02.08.23.out 2>&1

################
#dada2-pipeline#
################
path          <- "/work/16S/Batch1_Run1/trim_galore"
pathOut       <- "/work/16S/Batch1_Run1/02.08.23"
pathTrainsets <- "/work/16S/trainsets"
Date          <- "02.08.23"
Length        <- "470"

library(dada2)
library(ShortRead)
library(ggplot2)

head(list.files(path))
path.cut <- file.path(path, "cutadapt")
head(list.files(path.cut))
fnFs <- sort(list.files(path.cut, pattern = "R1_001_val_1.fq", full.names = TRUE))
fnRs <- sort(list.files(path.cut, pattern = "R2_001_val_2.fq", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
head(sample.names)

########
#Filter#
########
filtFs <- file.path(path.cut, paste("filtered", Length, sep="_"), paste0(sample.names, "_F_filt.fastq"))
filtRs <- file.path(path.cut, paste("filtered", Length, sep="_"), paste0(sample.names, "_R_filt.fastq"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs,
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE, verbose=TRUE,truncLen=c(270,200))
out

saveRDS(out, file.path(pathOut,paste(Date, Length, "out.rds", sep="_")))

filtFs <- filtFs[file.exists(filtFs)]
filtRs <- filtRs[file.exists(filtRs)]

###################
#Learn Error Rates#
###################
errF <- learnErrors(filtFs, multithread=TRUE)
saveRDS(errF, file.path(pathOut, paste(Date, Length,  "errF.rds", sep="_")))
errR <- learnErrors(filtRs, multithread=TRUE)

saveRDS(errR, file.path(pathOut, paste(Date, Length,"errR.rds", sep="_")))
library(ggplot2)
A<- plotErrors(errF, nominalQ=TRUE)
ggsave(A, filename = paste(Date, Length, "errF.png", sep="_"),path= pathOut, device='png', dpi=300, width = 10,height = 10)
B<- plotErrors(errR, nominalQ=TRUE)
ggsave(B, filename = paste(Date, Length, "errR.png", sep="_"),path=pathOut, device='png', dpi=300, width = 10,height = 10)

###############
#Infering ASVs#
###############
errF<-readRDS(file.path(pathOut, paste(Date,Length,"errF.rds", sep="_")))
errR<-readRDS(file.path(pathOut, paste(Date,Length,"errR.rds", sep="_")))

#Inferring ASVs
dadaFs <- dada(filtFs, err=errF, multithread=TRUE, pool=TRUE)
saveRDS(dadaFs, file.path(pathOut, paste(Date,Length,"dadaFs.rds", sep="_")))
dadaFs<-readRDS(file.path(pathOut, paste(Date,Length,"dadaFs.rds", sep="_")))

dadaRs <- dada(filtRs, err=errR, multithread=TRUE, pool=TRUE)
saveRDS(dadaRs, file.path(pathOut, paste(Date,Length,"dadaRs.rds", sep="_")))
dadaRs<-readRDS(file.path(pathOut, paste(Date,Length,"dadaRs.rds", sep="_")))

dadaFs[[1]]

####################
#merge paired reads#
####################
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
saveRDS(mergers, file.path(pathOut, paste(Date,Length,"mergers.rds", sep="_")))
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
#Construct sequence table
seqtab <- makeSequenceTable(mergers)

dim(seqtab)
saveRDS(seqtab, file.path(pathOut, paste(Date,Length,"seqtab.rds", sep="_")))
seqtab<-readRDS(file.path(pathOut, paste(Date,Length,"seqtab.rds", sep="_")))
#Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

#################
#Remove chimeras#
#################
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim) #Check dimensions to see if the a good number of samples have made it into the table with a good number of reads

sum(seqtab.nochim)/sum(seqtab) #Check the proportion of reads that made it past the chimera check

saveRDS(seqtab.nochim, file.path(pathOut, paste(Date,Length,"seqtab.nochim.rds", sep="_")))
seqtab.nochim<-readRDS(file.path(pathOut, paste(Date,Length,"seqtab.nochim.rds", sep="_")))

##################################
#Track reads through the pipeline#
##################################

dadaFs  <-readRDS(file.path(pathOut, paste(Date, Length, "dadaFs.rds", sep="_")))
dadaRs  <-readRDS(file.path(pathOut, paste(Date, Length,"dadaRs.rds", sep="_")))
out     <-readRDS(file.path(pathOut, paste(Date, Length,"out.rds", sep="_")))
mergers <-readRDS(file.path(pathOut, paste(Date, Length,"mergers.rds", sep="_")))

getN <- function(x) sum(getUniques(x)) #See how many unique sequences there are? Very tired, will check later.
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim)) #As it says on the box
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim") #Set column names
rownames(track) <- sample.names #Label rownames for each sample
head(track) #Check to see the proportion of samples that made it through the whole process.
write.table(track, file.path(pathOut, paste(Date, Length,"Track_reads_through_pipeline.txt", sep="_")))
ReadNum<- read.table(file.path(pathOut, paste(Date, Length,"Track_reads_through_pipeline.txt", sep="_")))
write.csv2(track, file.path(pathOut, paste(Date, Length,"Track_reads_through_pipeline.csv", sep="_")), row.names = T, sep=";")

#################
#Assign Taxonomy#
#################
taxa_wSpecies <- assignTaxonomy(seqtab.nochim, file.path(pathTrainsets, "silva_nr99_v138.1_wSpecies_train_set.fa.gz"), multithread=TRUE)

#In this step species level is already included <- but only for long-reads reliable!
# Extensions: The dada2 package also implements a method to make species level assignments based on exact matching between ASVs and sequenced reference strains. Recent analysis suggests that exact matching (or 100% identity) is the only appropriate way to assign species to 16S gene fragments. Currently, species-assignment training fastas are available for the Silva and RDP 16S databases. To follow the optional species addition step, download the silva_species_assignment_v132.fa.gz file, and place it in the directory with the fastq files.
#https://benjjneb.github.io/dada2/assign.html#species-assignment
# Fast and appropriate species-level assignment from 16S data is provided by the assignSpecies method. assignSpecies uses exact string matching against a reference database to assign Genus species binomials. In short, query sequence are compared against all reference sequences that had binomial genus-species nomenclature assigned, and the genus-species of all exact matches are recorded and returned if it is unambiguous. Recent results indicate that exact matching (or 100% identity) with amplicon sequence variants (ASVs) is the only appropriate method for species-level assignment to high-throughput 16S amplicon data.

taxa <- assignTaxonomy(seqtab.nochim, file.path(pathTrainsets, "silva_nr99_v138.1_train_set.fa.gz"), multithread=TRUE)
taxa <- addSpecies(taxa, file.path(pathTrainsets, "silva_species_assignment_v138.1.fa.gz"))

#When you want to do this it is nessessary to in the first step NOT TAKE THE _wSpecies_train_set.fa.gz otherwise we have Species twice in the data

taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL #Superflous information, get rid
taxa.print # See if it looks good

saveRDS(taxa, file.path(pathOut, paste(Date,Length, "Taxa_Species.rds", sep="_")))
saveRDS(taxa_wSpecies, file.path(pathOut, paste(Date,Length, "Taxa_wSpecies.rds", sep="_")))
```

## 1.7 Merge Runs and assign taxonomy

I use the seqtabs from the previous step to determine chimeras from all data together and assign one taxonomy
Batch1 and Batch1_reseq have same sample names and are merged
Batch2 is added in the dataset as individual samples 

```{r}
###############################################################
#Step4 Merge the SeqTabs from different sequencing runs and do#
###############################################################
#!/bin/bash
#SBATCH --job-name=dada2-merge
#SBATCH --nodes=1
#SBATCH --tasks-per-node=16
#SBATCH --partition=big 
#SBATCH --time=7-00:00:00
#SBATCH --export=NONE
#SBATCH --error=dada2-merge-02.08.23.txt
#SBATCH --mail-user=raphael.koll@uni-hamburg.de
#SBATCH --mail-type=ALL
source /sw/batch/init.sh
source /usw/anaconda3/etc/profile.d/conda.sh
conda activate MambaDada2
cd /work/16S/merge-Batch1and2
R -q -f dada2-merge-02.08.23.R  > dada2-merge-02.08.23.out 2>&1

#################
#Create R script#
#################
    #Steps done before:
    #Cutadapt cut our Primer sequences used from Kiel
    #Trim-Galore cut out Illumina adapters (in about 30% of the reads)

path <- "/work/16S/merge-Batch1and2/02.08.23"
pathOut <- "/work/16S/merge-Batch1and2/02.08.23"
pathTrainsets <-"/work/16S/trainsets"
Date <- "02.08.23_ReseqMerge"

library(dada2)

#https://benjjneb.github.io/dada2/bigdata_paired.html
#https://github.com/benjjneb/dada2/issues/1406
st1 <-readRDS(file.path(pathOut, "Batch1_Run1_02.08.23_470_seqtab.rds"))
st2 <-readRDS(file.path(pathOut, "Batch1_Run1_ReSeq_02.08.23_470_seqtab.rds"))
st3 <-readRDS(file.path(pathOut, "Batch2_Run2_02.08.23_470_seqtab.rds"))

st.all <- mergeSequenceTables(st1, st2, st3,  repeats = "sum")

#################
#Remove chimeras#
#################
seqtab.nochim <- removeBimeraDenovo(st.all, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim) #Check dimensions to see if the a good number of samples have made it into the table with a good number of reads

sum(seqtab.nochim)/sum(st.all) #Check the proportion of reads that made it past the chimera check

saveRDS(seqtab.nochim, file.path(pathOut, paste(Date, "seqtab.nochim.rds", sep="")))


#################
#Assign Taxonomy#
#################

taxa <- assignTaxonomy(seqtab.nochim, file.path(pathTrainsets, "silva_nr99_v138.1_train_set.fa.gz"), multithread=TRUE)
taxa <- addSpecies(taxa, file.path(pathTrainsets, "silva_species_assignment_v138.1.fa.gz"))
saveRDS(taxa, file.path(pathOut, paste(Date, "merge-Taxa_Species.rds", sep="")))
#When you want to do this it is nessessary to in the first step NOT TAKE THE _wSpecies_train_set.fa.gz otherwise we have Species twice in the data


taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL #Superflous information, get rid
head(taxa.print) # See if it looks good

taxa_wSpecies <- assignTaxonomy(seqtab.nochim, file.path(pathTrainsets, "silva_nr99_v138.1_wSpecies_train_set.fa.gz"), multithread=TRUE)

#In this step species level is already included <- but only for long-reads reliable!
# Extensions: The dada2 package also implements a method to make species level assignments based on exact matching between ASVs and sequenced reference strains. Recent analysis suggests that exact matching (or 100% identity) is the only appropriate way to assign species to 16S gene fragments. Currently, species-assignment training fastas are available for the Silva and RDP 16S databases. To follow the optional species addition step, download the silva_species_assignment_v132.fa.gz file, and place it in the directory with the fastq files.
#https://benjjneb.github.io/dada2/assign.html#species-assignment
# Fast and appropriate species-level assignment from 16S data is provided by the assignSpecies method. assignSpecies uses exact string matching against a reference database to assign Genus species binomials. In short, query sequence are compared against all reference sequences that had binomial genus-species nomenclature assigned, and the genus-species of all exact matches are recorded and returned if it is unambiguous. Recent results indicate that exact matching (or 100% identity) with amplicon sequence variants (ASVs) is the only appropriate method for species-level assignment to high-throughput 16S amplicon data.

saveRDS(taxa_wSpecies, file.path(pathOut, paste(Date, "merge-Taxa_wSpecies.rds", sep="")))
````

## 1.8 ENA Submit

```{r}
#####
#16S#
#####
for file in *_gillswab_R1.fastq.gz; do
    # Extract name without the "_gill_R1.fastq.gz" part
    name=$(echo "$file" | sed 's/_gillswab_R1.fastq.gz//')
    # Create Manifest file
    manifest_file="Manifest-${name}_gillswab.txt"
    # Write information to the Manifest file
    echo "STUDY PRJEB71116" > "$manifest_file"
    echo "SAMPLE ${name}_gillswab" >> "$manifest_file"
    echo "INSTRUMENT Illumina MiSeq" >> "$manifest_file"
    echo "NAME ${name}_gillswab" >> "$manifest_file"
    echo "LIBRARY_SOURCE OTHER" >> "$manifest_file"
    echo "LIBRARY_SELECTION unspecified" >> "$manifest_file"
    echo "LIBRARY_STRATEGY AMPLICON" >> "$manifest_file"
    echo "FASTQ ${name}_gillswab_R1.fastq.gz" >> "$manifest_file"
    echo "FASTQ ${name}_gillswab_R2.fastq.gz" >> "$manifest_file"
    echo "Manifest created for $name"
done

##########################
#Add technical replicates#
##########################
#Manifest-SLSU21MGEB7_gillswab.txt
STUDY PRJEB71116
SAMPLE SLSU21MGEB7_gillswab
INSTRUMENT Illumina MiSeq
NAME SLSU21MGEB7_gillswab
LIBRARY_SOURCE OTHER
LIBRARY_SELECTION unspecified
LIBRARY_STRATEGY AMPLICON
FASTQ SLSU21MGEB7_gillswab_batch1_R1.fastq.gz
FASTQ SLSU21MGEB7_gillswab_batch1_R2.fastq.gz
FASTQ SLSU21MGEB7_gillswab_batch1_reseq_R1.fastq.gz
FASTQ SLSU21MGEB7_gillswab_batch1_reseq_R2.fastq.gz

############
#ENA SUBMIT#
############
#!/bin/bash
#SBATCH --job-name=ENA-submit-16S
#SBATCH --nodes=1
#SBATCH --tasks-per-node=16
#SBATCH --partition=std
#SBATCH --time=12:00:00
#SBATCH --export=NONE
#SBATCH --error=error_ENA-submit-16S
#SBATCH --mail-user=raphael.koll@uni-hamburg.de
#SBATCH --mail-type=ALL
source /sw/batch/init.sh
module load java/oracle-jdk8u66
cd /work/Sander/SL_RNAseqs
for file in Manifest-*gillswab.txt; do
    java -jar /work/Sander/SL_RNAseqs/webin-cli-6.7.2.jar -userName=Webin-xxxxxx -password 'xxxx' -context reads -manifest="$file" -submit
done
```

#-
# 2 RNAseq


## 2.1. FastQC & MultiQC

```{r}
########
#FASTQC#
########

#!/bin/bash
#SBATCH --job-name=fastq-SanderLiver
#SBATCH --nodes=1
#SBATCH --tasks-per-node=16
#SBATCH --partition=std
#SBATCH --time=12:00:00
#SBATCH --export=NONE
#SBATCH --error=error_fastq-SanderLiver.txt
#SBATCH --mail-user=raphael.koll@uni-hamburg.de
#SBATCH --mail-type=ALL
source /sw/batch/init.sh
source /usw/anaconda3/etc/profile.d/conda.sh
conda activate MambaRNAseq
fastqc --outdir /work/Sander/GC_TrimmedReads/fastqc_GC_Liver --threads 20 /work/Sander/GC_TrimmedReads/*fq.gz

##########
#Multi QC#
##########
#launch Multiqc to create a report in html containing the whole of informations generated by FastQC
source /usw/anaconda3/etc/profile.d/conda.sh
conda activate MambaRNAseq
multiqc . #straigh in the folder, it searches itself for reports
```


## 2.2 Trimming

Some thoughts in trimming:

Liao, Y., & Shi, W. (2020). Read trimming is not required for mapping and quantification of RNA-seq reads at the gene level. NAR genomics and bioinformatics, 2(3), lqaa068.
https://dnatech.genomecenter.ucdavis.edu/faqs/when-should-i-trim-my-illumina-reads-and-how-should-i-do-it/
When should I trim my Illumina reads and how should I do it?
Should I trim adapters from my Illumina reads?
This depends on the objective of your experiments.
In case you are sequencing for counting applications like differential gene expression (DGE) RNA-seq analysis, ChIP-seq, ATAC-seq, read trimming is generally not required anymore when using modern aligners.  For such studies local aligners or pseudo-aligners should be used. Modern âlocal alignersâ like STAR, BWA-MEM, HISAT2, will âsoft-clipâ non-matching sequences. Pseudo-aligners like Kallisto or Salmon will also not have any problem with reads containing adapter sequences.
However, if the data are used for variant analyses, genome annotation or genome or transcriptome assembly purposes, we recommend read trimming, including both, adapter and quality trimming.
How should I adapter trim my Illumina reads?
Paired-end-read sequencing data should be trimmed using algorithms that make use of the paired-end nature to enable the most precise trimming. This mode will not require any knowledge of the adapter sequences.
Recommended tools would be for example these tools in their dedicated paired-end modes:  BBduk, Skewer, HTStream, FASTP.  Among these, Skewer is likely the tool that is the easiest to use.
Trimming of single-end-read sequencing data requires knowledge of the adapter sequences (please see below). Recommended tools would be Scythe, Cutadapt, and Trimmomatic, HTStream, BBduk in their single-end modes.
DNA and RNA sequencing:
For small RNA/miRNA sequencing data please use this sequence bu also see this FAQ: How should the miRNA/smallRNA data be trimmed?.
TruSeq Small RNA: TGGAATTCTCGGGTGCCAAGG

Please see also this page from Illumina: What sequences do I use for adapter trimming?
Quality trimming:
The counting applications the same considerations as for adapter trimming (above) apply for quality trimming. It can be omitted if using the right aligners.
For other applications, we recommend to combine gentle quality trimming with a threshold quality score of Q15 with a read length filter retaining only reads longer than 35 bp in length.
Quality trimming tools: e.g. Sickle, Trimmomatic, HTStream, BBduk.
References:
Williams et al. 2016. Trimming of sequence reads alters RNA-Seq gene expression estimates. BMC Bioinformatics. 2016;17:103. Published 2016 Feb 25. doi:10.1186/s12859-016-0956-2

```{r}
#############
#trim-galore#
#############
#conda create --name CondaTrim trim-galore
#conda install -c conda-forge openmpi
#trim_galore --paired SAMPLE_R1.fastq.gz SAMPLE_R2.fastq.gz

#For-Loopl-Job
#!/bin/bash
#SBATCH --job-name=Trim
#SBATCH --nodes=1
#SBATCH --partition=big
#SBATCH --tasks-per-node=16
#SBATCH --time=7-00:00:00
#SBATCH --export=NONE
#SBATCH --error=error_Trim.txt
#SBATCH --mail-user=raphael.koll@uni-hamburg.de
#SBATCH --mail-type=ALL
source /sw/batch/init.sh
source /usw/anaconda3/etc/profile.d/conda.sh
conda activate CondaTrim
for f1 in *R1_001.fastq.gz
do
        f2=${f1%%R1_001.fastq.gz}"R2_001.fastq.gz"
        trim_galore --paired --clip_R1 1 --clip_R2 1 --fastqc --core 16 -o trim_galore/ $f1 $f2
done

##########
#Evaluate#
##########
ls -dq *fq.gz | wc -l
##########
#Multi QC#
##########
#launch Multiqc to create a report in html containing the whole of informations generated by FastQC
source /usw/anaconda3/etc/profile.d/conda.sh
conda activate MambaRNAseq
multiqc . #straigh in the folder, it searches itself for reports
```

## 2.3 HISAT2

```{r}
########
#Hisat2#
########

#######
#Setup#
#######
#Short explanation:
    #wget genome.fna.gz, genome.gtf.gz and unzip
    #Index genome.fna
    #Executable Batch runHisat.sh: hsat on Index files of genome.fna and fastq.gz files
    #Loop for all samples in the folder hisat2RK.cmds: bash ./runHisat.sh R1.fastq.gz R2.fastq.gz

#https://www.biostars.org/p/404914/
#https://stackoverflow.com/questions/58597789/hisat2-with-job-array
#https://gif.biotech.iastate.edu/rnaseq-analysis-walk-through
#https://njstem.wordpress.com/2016/10/07/tuxedo2-pipeline-on-perceval/

#########Step 0:load and unzip fna.gz
mkdir SLUC_FBN
wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/008/315/115/GCF_008315115.2_SLUC_FBN_1.2/GCF_008315115.2_SLUC_FBN_1.2_genomic.gtf.gz
wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/008/315/115/GCF_008315115.2_SLUC_FBN_1.2/GCF_008315115.2_SLUC_FBN_1.2_genomic.fna.gz
unzip GCF_008315115.2_SLUC_FBN_1.2_genomic.gt
unzip GCF_008315115.2_SLUC_FBN_1.2_genomic.fna.gz

https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/021/917/145/GCF_021917145.1_fHypTra1/GCF_021917145.1_fHypTra1_genomic.gbff.gz

#########Step 1: Make Index file from fna.gz
#https://gif.biotech.iastate.edu/rnaseq-analysis-walk-through
mkdir index genes genome

#!/bin/bash
#SBATCH --job-name=SLUC_index
#SBATCH --nodes=1
#SBATCH --partition=big
#SBATCH --tasks-per-node=20
#SBATCH --time=12:00:00
#SBATCH --export=NONE
#SBATCH --error=error_SLUC_index.txt
#SBATCH --mail-user=raphael.koll@uni-hamburg.de
#SBATCH --mail-type=ALL
source /usw/anaconda3/etc/profile.d/conda.sh
conda activate MambaRNAseq
hisat2-build -p 16 /work/hisat2/SLUC_FBN/index/GCF_008315115.2_SLUC_FBN_1.2_genomic.fna /work/hisat2/SLUC_FBN/index/SLUC_Hisat2


###############
#Hisat2 Script#
###############

###############################################################################
#Loop to create samplefile and plit into 10 line pieces to allocate 1 per node#
###############################################################################
for fastq in *R1_001.fq.gz; do
fastq2=$(echo $fastq | sed 's/R1_001.fq.gz/R2_001.fq.gz/g');
echo "bash runHISAT2.sh ${fastq} ${fastq2}";
done > hisat2.cmds
split -l 10 --numeric-suffixes hisat2.cmds splithisat2.cmds

######################################
#!/bin/bash
#SBATCH --job-name=HisatArray-09.06.23-00
#SBATCH --nodes=1
#SBATCH --tasks-per-node=16
#SBATCH --partition=std #64 gb ram
#SBATCH --time=12:00:00
#SBATCH --export=NONE
#SBATCH --error=error_HisatArray-09.06.23.txt-00
#SBATCH --mail-user=raphael.koll@uni-hamburg.de
#SBATCH --mail-type=ALL
#SBATCH --array=1-10
source /sw/batch/init.sh

workingdir="/work/Sander/Gill"                 
list=/work/Sander/Gill/splithisat2.cmds00  

string="sed -n "$SLURM_ARRAY_TASK_ID"p ${list}"
str=$($string)

var=$(echo $str | awk -F"\t" '{print $1}')
set -- $var
c1=$1
c2=$2
c3=$3
c4=$4

echo "$c3"

source /usw/anaconda3/etc/profile.d/conda.sh
conda activate MambaRNAseq

DBDIR="/work/hisat2/SLUC_FBN/index"
GENOME="SLUC_Hisat2"

p=16
R1_FQ="$c3"
R2_FQ="$c4"
OUTPUT=$(basename ${R1_FQ}  | cut -f1-3 -d"_");
hisat2 \
  --threads ${p} \
  --very-sensitive \
  --rna-strandness RF \
  --new-summary \
  --summary-file \
  -x ${DBDIR}/${GENOME} \
  -1 ${R1_FQ} \
  -2 ${R2_FQ} | \
  tee >(samtools flagstat - > ${workingdir}/${OUTPUT}_output.flagstat) | \
  samtools sort -O BAM | \
  tee ${workingdir}/${OUTPUT}.sorted.bam | \
  samtools index - ${workingdir}/${OUTPUT}.bam.bai

##########
#Evaluate#
##########
ls -l | grep -c '\.bam$'
for file in *.flagstat; do grep -n '' "$file" | sed -n '7s/^/'"$file"' /p'; done > hisat2mappingrates.txt
for file in *Gill_trimmed_output.flagstat; do grep -n '' "$file" | sed -n '7s/^/'"$file"' /p'; done > SL_Gill_hisat2mappingrates.txt
for file in *Gill_trimmed_output.flagstat; do grep -n '' "$file" | sed -n '7s/^/'"$file"' /p'; done > SL_Gill_hisat2mappingrates.txt


##################
#Hisat Individual# for files that exceeded 64GB ram and collapsed
##################
#!/bin/bash
#SBATCH --job-name=Hisat2
#SBATCH --nodes=1
#SBATCH --tasks-per-node=16
#SBATCH --partition=big #256 gb ram
#SBATCH --time=7-00:00:00
#SBATCH --export=NONE
#SBATCH --error=error_Hisat2-SLSU21BBEB4
#SBATCH --mail-user=raphael.koll@uni-hamburg.de
#SBATCH --mail-type=ALL
source /sw/batch/init.sh
source /usw/anaconda3/etc/profile.d/conda.sh
conda activate MambaRNAseq
DBDIR="/work/hisat2/SLUC_FBN/index"
GENOME="SLUC_Hisat2"
R1_FQ="/work/Sander/Gill/SLSU21BBEB4_Gill_trimmed_R1_001.fq.gz"
R2_FQ="/work/Sander/Gill/SLSU21BBEB4_Gill_trimmed_R2_001.fq.gz"
OUTPUT=$(basename ${R1_FQ}  | cut -f1-3 -d"_");
hisat2 \
  --threads 16 \
  --very-sensitive \
  --rna-strandness RF \
  --new-summary \
  --summary-file \
  -x ${DBDIR}/${GENOME} \
  -1 ${R1_FQ} \
  -2 ${R2_FQ} | \
  tee >(samtools flagstat - > ${OUTPUT}_output.flagstat) | \
  samtools sort -O BAM | \
  tee ${OUTPUT}.sorted.bam | \
  samtools index - ${OUTPUT}.bam.bai

##########
#Multi QC#
##########
#launch Multiqc to create a report in html containing the whole of informations generated by FastQC
source /usw/anaconda3/etc/profile.d/conda.sh
conda activate MambaRNAseq
multiqc . #straigh in the folder, it searches itself for reports
```

## 2.4 featureCounts

```{r}
######################################################################################################
################### featureCounts Abundence estimation ###############################################
######################################################################################################
#featureCounts is a highly efficient general-purpose read summarization program that counts mapped reads for genomic features such as genes, exons, promoter, gene bodies, genomic bins and chromosomal locations. featureCounts takes as input SAM/BAM files and an annotation file including chromosomal coordinates of features. It outputs numbers of reads assigned to features (or meta-features). It also outputs stat info for the overall summrization results, including number of successfully assigned reads and number of reads that failed to be assigned due to various reasons (these reasons are included in the stat info). We can run this on all SAM/BAM files at the same time.

################
#One time setup#
################
conda install -c bioconda subread #FeatureCounts Version 2.0.1
conda install -c conda-forge unzip
gunzip GCF_008315115.2_SLUC_FBN_1.2_genomic.gtf.gz
/work/hisat2/SLUC_FBN/genes/GCF_008315115.2_SLUC_FBN_1.2_genomic.gtf

#Solve the Error with GTF/GFF3 Gene ID in Column 9 not there:
#https://www.biostars.org/p/432735/
grep 'gene_id ""' SLUC.gtf
grep -v 'gene_id ""' SLUC.gtf > SLUC_fixed.gtf

###################
#run FeatureCounts#
###################
#!/bin/bash
#SBATCH --job-name=featureCounts-SL_Gill
#SBATCH --nodes=1
#SBATCH --tasks-per-node=16
#SBATCH --partition=std
#SBATCH --time=12:00:00
#SBATCH --export=NONE
#SBATCH --error=error_featureCounts-SL_Gill_22.06.23
#SBATCH --mail-user=raphael.koll@uni-hamburg.de
#SBATCH --mail-type=ALL
source /sw/batch/init.sh
source /usw/anaconda3/etc/profile.d/conda.sh
conda activate MambaRNAseq

GFF="/work/featurecounts/SLUC_fixed.gtf"
OUTFILE="featureCounts_SL_Gill"
featureCounts -a ${GFF} -T 16 -p -t exon -s 2 -o ${OUTFILE}_counts.txt /work/Sander/Gill/*.bam


########################
#Swith to R with Output#
########################
```

## 2.5 ENA upload

```{r}
###############
#Upload to ENA#
###############
wget https://github.com/enasequence/webin-cli/releases/download/6.7.2/webin-cli-6.7.2.jar
module load java/oracle-jdk8u66

#########
#RNAseqs#
#########
for file in *_Gill_R1.fastq.gz; do
    # Extract name without the "_gill_R1.fastq.gz" part
    name=$(echo "$file" | sed 's/_Gill_R1.fastq.gz//')
    # Create Manifest file
    manifest_file="Manifest-${name}_Gill.txt"
    # Write information to the Manifest file
    echo "STUDY PRJEB71116" > "$manifest_file"
    echo "SAMPLE ${name}_Gill" >> "$manifest_file"
    echo "INSTRUMENT Illumina NovaSeq 6000" >> "$manifest_file"
    echo "NAME ${name}_Gill" >> "$manifest_file"
    echo "LIBRARY_SOURCE TRANSCRIPTOMIC" >> "$manifest_file"
    echo "LIBRARY_SELECTION Oligo-dT" >> "$manifest_file"
    echo "LIBRARY_STRATEGY RNA-Seq" >> "$manifest_file"
    echo "FASTQ ${name}_Gill_R1.fastq.gz" >> "$manifest_file"
    echo "FASTQ ${name}_Gill_R2.fastq.gz" >> "$manifest_file"
    echo "Manifest created for $name"
done

#!/bin/bash
#SBATCH --job-name=ENA-submit-gillRNAseq
#SBATCH --nodes=1
#SBATCH --tasks-per-node=16
#SBATCH --partition=stl
#SBATCH --time=7-00:00:00
#SBATCH --export=NONE
#SBATCH --error=error_ENA-submit-gillRNAseq
#SBATCH --mail-user=raphael.koll@uni-hamburg.de
#SBATCH --mail-type=ALL
source /sw/batch/init.sh
module load java/oracle-jdk8u66
cd /work/Sander/SL_RNAseqs
for file in Manifest-*gill.txt; do
    java -jar /work/Sander/SL_RNAseqs/webin-cli-6.7.2.jar -userName=Webin-xxxxxx -password 'xxxxxx' -context reads -manifest="$file" -submit
done
```

#-

```{r}
